{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in /opt/conda/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.6/site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from keras) (1.14.0)\n",
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow_gpu-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8 MB)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.0.8)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.14.0)\n",
      "Collecting numpy<2.0,>=1.16.0\n",
      "  Using cached numpy-1.18.2-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (3.11.3)\n",
      "Processing /home/jovyan/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63/wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Using cached scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1 MB)\n",
      "Processing /home/jovyan/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp36-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-gpu) (0.9.0)\n",
      "Processing /home/jovyan/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6/termcolor-1.1.0-cp36-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (0.34.2)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.2.0-py3-none-any.whl (63 kB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.1.0)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Using cached tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorflow-gpu) (1.27.2)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.0.0.post20200311)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /opt/conda/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.11.3)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /home/jovyan/.local/lib/python3.6/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home/jovyan/.local/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /home/jovyan/.local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
      "Installing collected packages: google-pasta, numpy, wrapt, scipy, gast, termcolor, opt-einsum, tensorflow-estimator, tensorboard, astor, tensorflow-gpu\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.1.0\n",
      "    Uninstalling scipy-1.1.0:\n",
      "      Successfully uninstalled scipy-1.1.0\n",
      "Successfully installed astor-0.8.1 gast-0.2.2 google-pasta-0.2.0 numpy-1.18.2 opt-einsum-3.2.0 scipy-1.4.1 tensorboard-2.1.1 tensorflow-estimator-2.1.0 tensorflow-gpu-2.1.0 termcolor-1.1.0 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U keras\n",
    "!pip install -U tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(textfile):\n",
    "    # Open the file as read-only\n",
    "    file = open(textfile, 'r')\n",
    "    # Actually read the file\n",
    "    text = file.read()\n",
    "    # Now that I have the data in my text variable, need to close out of the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = loader('../data/sequence_reviews.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'talk about wow if anyone ever'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to convert the words in my data to integers in order to be used by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def word_to_int(lines):\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    sequences = tokenizer.texts_to_sequences(data)\n",
    "    # Need the total size of the vocabulary for our embedding layer in model\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return sequences, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_data, vocab_size = word_to_int(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove last empty row if any\n",
    "if len(sequence_data[-1]) == 0:\n",
    "    sequence_data = sequence_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_array = np.array(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sequence_array[:,:-1], sequence_array[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"basic-lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 5, 50)             1882100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 37642)             3801842   \n",
      "=================================================================\n",
      "Total params: 5,834,842\n",
      "Trainable params: 5,834,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# defining my model\n",
    "\n",
    "model = Sequential(name=\"basic-lstm\")\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"model/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "670274/670274 [==============================] - 291s 434us/step - loss: 6.9139 - accuracy: 0.0694\n",
      "Epoch 2/20\n",
      "670274/670274 [==============================] - 252s 375us/step - loss: 6.2916 - accuracy: 0.1122\n",
      "Epoch 3/20\n",
      "670274/670274 [==============================] - 258s 385us/step - loss: 5.9441 - accuracy: 0.1274\n",
      "Epoch 4/20\n",
      "670274/670274 [==============================] - 250s 373us/step - loss: 5.7082 - accuracy: 0.1378\n",
      "Epoch 5/20\n",
      "670274/670274 [==============================] - 257s 384us/step - loss: 5.5246 - accuracy: 0.1463\n",
      "Epoch 6/20\n",
      "670274/670274 [==============================] - 251s 374us/step - loss: 5.3666 - accuracy: 0.1538\n",
      "Epoch 7/20\n",
      "670274/670274 [==============================] - 255s 381us/step - loss: 5.2265 - accuracy: 0.1607\n",
      "Epoch 8/20\n",
      "670274/670274 [==============================] - 251s 374us/step - loss: 5.0986 - accuracy: 0.1675\n",
      "Epoch 9/20\n",
      "670274/670274 [==============================] - 256s 382us/step - loss: 4.9799 - accuracy: 0.1738\n",
      "Epoch 10/20\n",
      "670274/670274 [==============================] - 250s 373us/step - loss: 4.8683 - accuracy: 0.1800\n",
      "Epoch 11/20\n",
      "670274/670274 [==============================] - 259s 386us/step - loss: 4.7619 - accuracy: 0.1859\n",
      "Epoch 12/20\n",
      "670274/670274 [==============================] - 251s 374us/step - loss: 4.6626 - accuracy: 0.1923\n",
      "Epoch 13/20\n",
      "670274/670274 [==============================] - 257s 383us/step - loss: 4.5674 - accuracy: 0.1988\n",
      "Epoch 14/20\n",
      "670274/670274 [==============================] - 251s 374us/step - loss: 4.4779 - accuracy: 0.2054\n",
      "Epoch 15/20\n",
      "670274/670274 [==============================] - 257s 383us/step - loss: 4.3942 - accuracy: 0.2123\n",
      "Epoch 16/20\n",
      "670274/670274 [==============================] - 251s 374us/step - loss: 4.3160 - accuracy: 0.2194\n",
      "Epoch 17/20\n",
      "530432/670274 [======================>.......] - ETA: 53s - loss: 4.2175 - accuracy: 0.2286"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
